{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c07066e",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d609faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41c06c5",
   "metadata": {},
   "source": [
    "# Load the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af7dd5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the path to the directory containing the images\n",
    "path = \"C:/Users/danie/Desktop/NTU/Modules/Year 4 Semester 2/Intelligence System/Assignment/facerecognition-main/facerecognition-main/FR\"\n",
    "\n",
    "labels = ['raj','stan', 'daniel', 'jack','weimin']\n",
    "# Load the images and labels into arrays\n",
    "# images = []\n",
    "# image_labels = []\n",
    "# for label in labels:\n",
    "#     image_paths = [os.path.join(path, label, f) for f in os.listdir(os.path.join(path, label)) if f.endswith('.jpg')]\n",
    "#     for image_path in image_paths:\n",
    "#         img = cv2.imread(image_path,0)\n",
    "#         img = cv2.resize(img, (200, 200)) # Resize the image to 200 x 200 pixels\n",
    "#         images.append(img)\n",
    "#         image_labels.append(label)\n",
    "\n",
    "# # Convert the image and label arrays to NumPy arrays\n",
    "# images = np.array(images)\n",
    "# image_labels = np.array(image_labels)\n",
    "\n",
    "# Load the images and labels into arrays\n",
    "images = []\n",
    "image_labels = []\n",
    "for label in labels:\n",
    "    image_paths = [os.path.join(path, label, f) for f in os.listdir(os.path.join(path, label)) if f.endswith('.jpg')]\n",
    "    for image_path in image_paths:\n",
    "        img = cv2.imread(image_path, 0)\n",
    "        img = cv2.resize(img, (200, 200)) # Resize the image to 200 x 200 pixels\n",
    "\n",
    "        # Apply data augmentation to the image\n",
    "        # Randomly rotate the image by a small angle\n",
    "        angle = np.random.randint(-10, 10)\n",
    "        rows, cols = img.shape\n",
    "        M = cv2.getRotationMatrix2D((cols/2, rows/2), angle, 1)\n",
    "        img = cv2.warpAffine(img, M, (cols, rows))\n",
    "\n",
    "        # Randomly flip the image horizontally\n",
    "        if np.random.rand() < 0.5:\n",
    "            img = cv2.flip(img, 1)\n",
    "\n",
    "        images.append(img)\n",
    "        image_labels.append(label)\n",
    "\n",
    "# Convert the image and label arrays to NumPy arrays\n",
    "images = np.array(images)\n",
    "image_labels = np.array(image_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9b8612",
   "metadata": {},
   "source": [
    "# Normalize and make it One Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0774bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 40000)\n"
     ]
    }
   ],
   "source": [
    "#Normalize the image array, the image vector values will be between 0 and 1. Convert to numpy array to perform mathematical expressions.\n",
    "\n",
    "#convert the image into 1D array, \n",
    "image_vectors = np.array([img.flatten() for img in images])\n",
    "print(image_vectors.shape)\n",
    "\n",
    "mean = np.mean(image_vectors, axis=0)\n",
    "std = np.std(image_vectors, axis=0)\n",
    "\n",
    "# normalize the image vectors by subtracting the mean and dividing by the standard deviation\n",
    "image_vectors_norm = (image_vectors - mean) / std\n",
    "\n",
    "# image_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c116f75",
   "metadata": {},
   "source": [
    "# Perform PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f9fff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a PCA object\n",
    "# pca = PCA(n_components=31)  # set the number of components you want to keep\n",
    "\n",
    "# # fit the PCA model to the image vectors\n",
    "# pca.fit(image_vectors)\n",
    "\n",
    "# # transform the image vectors to their low-dimensional representation\n",
    "# image_vectors_transformed = pca.transform(image_vectors)\n",
    "# print(image_vectors_transformed.shape)\n",
    "# # plot the transformed data with the first two principal components as the x and y axes\n",
    "# plt.scatter(image_vectors_transformed[:, 0], image_vectors_transformed[:, 1])\n",
    "\n",
    "# # plot the first principal component as a line passing through the scatter plot\n",
    "# x = np.linspace(-5, 5)\n",
    "# y = x * pca.components_[0, 1] / pca.components_[0, 0]\n",
    "# plt.plot(x, y, color='r')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4876c572",
   "metadata": {},
   "source": [
    "# Perform LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe3ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "color_dict = {\n",
    "              'raj': 'blue',\n",
    "              'stan' : 'red',\n",
    "              'daniel' : 'cyan',\n",
    "              'jack' : 'orange',\n",
    "              'weimin' : 'purple'\n",
    "           }\n",
    "# create an LDA object\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "\n",
    "# fit the LDA model to the image vectors\n",
    "lda.fit(image_vectors, image_labels)\n",
    "\n",
    "# transform the image vectors to their low-dimensional representation\n",
    "image_vectors_transformed = lda.transform(image_vectors)\n",
    "\n",
    "colors = [color_dict[label] for label in image_labels]\n",
    "\n",
    "\n",
    "# create a scatter plot of the transformed image vectors, using the image_labels to assign a unique color to each label\n",
    "plt.scatter(image_vectors_transformed[:, 0], image_vectors_transformed[:, 1], c=colors)\n",
    "plt.xlabel('LDA Component 1')\n",
    "plt.ylabel('LDA Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01fb8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Create a list of the number of components to try\n",
    "# n_components_list = [1, 2, 3, 4, 5]\n",
    "\n",
    "# # Create a KFold object for 5-fold cross-validation\n",
    "# kf = KFold(n_splits=5)\n",
    "\n",
    "# # Loop over each value of n_components\n",
    "# for n_components in n_components_list:\n",
    "#     # Create an LDA object with the current number of components\n",
    "#     lda = LinearDiscriminantAnalysis(n_components=n_components)\n",
    "\n",
    "#     # Initialize a list to store the accuracy for each fold\n",
    "#     accuracies = []\n",
    "\n",
    "#     # Loop over each fold of the data\n",
    "#     for train_idx, test_idx in kf.split(image_vectors):\n",
    "#         # Split the data into training and test sets\n",
    "#         X_train, X_test = image_vectors[train_idx], image_vectors[test_idx]\n",
    "#         y_train, y_test = image_labels[train_idx], image_labels[test_idx]\n",
    "\n",
    "#         # Fit the LDA model to the training data and transform the test data\n",
    "#         lda.fit(X_train, y_train)\n",
    "#         X_test_transformed = lda.transform(X_test)\n",
    "\n",
    "#         # Predict the labels for the test data and compute the accuracy\n",
    "#         y_pred = lda.predict(X_test_transformed)\n",
    "#         accuracy = accuracy_score(y_test, y_pred)\n",
    "#         accuracies.append(accuracy)\n",
    "\n",
    "#     # Compute the average accuracy across all folds\n",
    "#     mean_accuracy = np.mean(accuracies)\n",
    "\n",
    "#     # Print the number of components and the mean accuracy\n",
    "#     print(\"n_components: {}, accuracy: {:.3f}\".format(n_components, mean_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f1dfe",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4ab9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_vectors_transformed, image_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a kNN classifier with k=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Print the accuracy of the classifier\n",
    "print(\"Accuracy:\", np.mean(y_pred == y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b435a81",
   "metadata": {},
   "source": [
    "# Viola Jones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a7b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the default camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "N = 10\n",
    "last_faces = []\n",
    "\n",
    "while True:\n",
    "    # Read the next frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame using the Haar cascade classifier\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # last_faces.append(faces)\n",
    "    # if len(last_faces) > N:\n",
    "    #     last_faces.pop(0)\n",
    "\n",
    "    # if last_faces:\n",
    "    #     x = sum([face[0] for faces in last_faces for face in faces]) // len(last_faces)\n",
    "    #     y = sum([face[1] for faces in last_faces for face in faces]) // len(last_faces)\n",
    "    #     w = sum([face[2] for faces in last_faces for face in faces]) // len(last_faces)\n",
    "    #     h = sum([face[3] for faces in last_faces for face in faces]) // len(last_faces)\n",
    "\n",
    "        # For each detected face, recognize the person and draw a rectangle around the face\n",
    "    for (x, y, w, h) in faces:\n",
    "            # Crop the detected face from the frame and resize it to the same size as the face images used during training\n",
    "            face = gray[y:y+h, x:x+w]\n",
    "            face_resized = cv2.resize(face, (200, 200))\n",
    "\n",
    "            # Apply the same PCA transformation to the cropped and resized face to get the feature vector for that face\n",
    "            face_vector = face_resized.flatten().reshape(1, -1)\n",
    "    #         face_vector_transformed = pca.transform(face_vector)\n",
    "            face_vector_transformed = lda.transform(face_vector)\n",
    "\n",
    "            # Use the kNN classifier to predict the identity of the person based on the feature vector\n",
    "            predicted_label = knn.predict(face_vector_transformed)[0]\n",
    "\n",
    "            # Draw a rectangle around the detected face and display the predicted label on the rectangle\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, predicted_label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Recognition\", frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13dc07b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b12d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700657dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ff4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
